\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{setspace}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}

\onehalfspacing

\title{Exploratory Computational Models Towards Understanding Human Decision-Making: Signal Choice and Group Structure in Norm Formation}
\author{Yifan Wu}
\date{}

\begin{document}

\maketitle

\section{Introduction}

Human collective endeavors require effective coordination, yet groups struggle to converge on shared strategies through poorly understood mechanisms. Both group size and signaling possibilities influence coordination outcomes, but their interaction with different individual learning approaches remains inadequately characterized.

We systematically dissect these factors using agent-based simulations of a pure coordination game. Our agents navigated coordination challenges across varying group sizes (N=2-20) under three distinct communication protocols: no signals, mandatory truthful signals, or optional signals. Agents employed either historical success tracking or reinforcement learning as their adaptive strategy. This computational investigation establishes a framework for understanding coordination dynamics by identifying critical thresholds between group size, signal freedom, and learning type, generating specific hypotheses for subsequent empirical testing in human groups.

\section{Materials and Methods}
\paragraph{Simulation Framework}
We implemented a dyadic, pure coordination game with binary choice mechanisms in our agent-based simulation. Agents faced a simple decision between 'red' or 'blue' options, aiming to match their partner's choice. Group size varied systematically across eight levels (N = 2, 3, 4, 6, 8, 10, 16, and 20) to examine scaling effects on coordination dynamics.

Interactions occurred exclusively between agent pairs within their designated groups. Coordination success required both agents to select identical colors, while system-level convergence demanded unanimous color selection across all agents. Simulations continued until either complete convergence occurred or the maximum limit of 100,000 rounds was reached.

\paragraph{Agent Learning Strategies}
We implemented two distinct learning strategies to explore different cognitive approaches to coordination:
\begin{enumerate}
    \item \textbf{History-Based (HB) Agents}: These agents maintained records of historical success rates for each color choice. Their decision process probabilistically favored colors with higher recorded success rates in subsequent interactions.
    
    \item \textbf{Reward-Based (RB) Agents}: These agents employed reinforcement learning principles to adapt their behavior. They updated color selection probabilities after every interaction based on outcomes, following a standard delta rule that strengthened successful choices.
\end{enumerate}

\paragraph{Communication Conditions} 
We implemented three communication protocols to investigate how information exchange affects coordination:
\begin{enumerate}
    \item \textbf{No Signal (NS)}: Agents selected actions without prior communication, relying solely on their learning mechanisms to guide choices.
    
    \item \textbf{Mandatory Signal (MS)}: Agents truthfully signaled their intended choice to partners before each interaction round, providing perfect information about upcoming decisions.
    
    \item \textbf{Optional Signal (OS)}: Agents decided whether to signal their intended choice in this protocol. Their signaling decision updated dynamically based on their learning mechanism, adding strategic complexity to the coordination challenge.
\end{enumerate}

\paragraph{Experimental Design}
Our experimental design varied three key dimensions: group size (8 levels), agent learning strategy (HB vs. RB), and communication condition (NS, MS, OS). We performed 100 independent simulations for each unique condition combination to ensure statistical reliability. Each simulation run produced two primary outcome measures: convergence speed (rounds to unanimity) and ultimate success rate (proportion achieving convergence).

\section{Results}

Learning strategy dramatically influenced how group size affected coordination performance. Larger groups consistently converged more slowly across all conditions, but this effect manifested much more strongly in HB agents. History-based learners showed drastically reduced convergence rates in groups of N ≥ 8 under both NS and MS protocols. In contrast, RB agents maintained faster convergence and near-perfect success rates even in larger groups, demonstrating superior scalability of reinforcement-based approaches.

Communication protocols affected performance differently depending on the underlying learning strategy. Both MS and OS conditions facilitated significantly faster convergence for RB agents across all group sizes compared to the NS baseline. For HB agents, the MS condition unexpectedly hindered coordination as group size increased, while the OS condition substantially mitigated these detrimental effects. This interaction reveals that communication effectiveness depends critically on how agents process and respond to signals.

Reinforcement-based learning consistently outperformed history-based approaches across all experimental conditions, achieving convergence both faster and more reliably. This performance advantage grew increasingly pronounced with larger group sizes, highlighting fundamental limitations of history-based strategies in complex coordination networks.

\section{Discussion}

Reinforcement learning offers significant advantages over history-based strategies for coordination tasks in social systems. This advantage stems from reinforcement learning's direct association between actions and outcomes, providing greater robustness when navigating complex collective action problems in larger groups.

Communication and learning strategy interact in unexpected ways within coordination dynamics. Mandatory signaling hindered coordination for HB agents, potentially by locking them into suboptimal early choices. Optional signaling provided adaptive advantages by incorporating the signaling decision itself into the learning process alongside the primary coordination choice.

These signaling benefits varied across different contexts despite adding complexity to the coordination task. HB agents benefited from optional signaling when establishing initial local convergence patterns. However, converging simultaneously on both action and signaling policy sometimes hindered rapid global consensus, explaining why optional signaling, despite its theoretical advantages, proved suboptimal for RB agents in certain group sizes.

These computational findings raise fundamental questions about human coordination mechanisms in real-world settings. How human cognition aligns decisions about signaling versus acting, and how individuals integrate diverse signals within groups, represents key avenues for future empirical research comparing these computational predictions with actual human behavior.

\section{Conclusion}

Reinforcement learning provides superior scalability over history-based methods for group coordination challenges. Communication effectiveness depends crucially on the underlying learning strategy, with optional signaling offering adaptive benefits by integrating signaling decisions into the learning process. This interplay between reinforcement-like learning and adaptive communication reveals mechanisms potentially underlying large-scale human coordination across diverse social contexts.

\bibliographystyle{apalike}
\begin{thebibliography}{10}

\bibitem[Balliet(2010)]{Balliet2010}
Balliet, D. (2010).
\newblock Communication and cooperation in social dilemmas: A meta-analytic review.
\newblock \emph{Journal of Conflict Resolution, 54}(1), 39--57.

\bibitem[Barcelo and Capraro(2015)]{BarceloCapraro2015}
Barcelo, H., \& Capraro, V. (2015).
\newblock Group size effect on cooperation in one-shot social dilemmas.
\newblock \emph{Scientific Reports, 5}, 7937.

\bibitem[Baronchelli(2018)]{Baronchelli2018}
Baronchelli, A. (2018).
\newblock The emergence of consensus: A primer.
\newblock \emph{Royal Society Open Science, 5}(2), 172189.

\bibitem[Centola and Baronchelli(2015)]{Centola2015}
Centola, D., \& Baronchelli, A. (2015).
\newblock The spontaneous emergence of conventions: An experimental study of cultural evolution.
\newblock \emph{Proceedings of the National Academy of Sciences, 112}(7), 1989--1994.

\bibitem[Claus and Boutilier(1998)]{Claus1998}
Claus, B., \& Boutilier, C. (1998).
\newblock The dynamics of reinforcement learning in cooperative multiagent systems.
\newblock \emph{AAAI/IAAI}, 746--752.

\bibitem[Daw et al.(2005)]{Daw2005}
Daw, N. D., Niv, Y., \& Dayan, P. (2005).
\newblock Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control.
\newblock \emph{Nature Neuroscience, 8}(12), 1704--1711.

\bibitem[Flache et al.(2017)]{Flache2017}
Flache, A., Mäs, M., Feliciani, T., Chattoe-Brown, E., Deffuant, G., Huet, S., \& Lorenz, J. (2017).
\newblock Models of social influence: Towards the next frontiers.
\newblock \emph{Journal of Artificial Societies and Social Simulation, 20}(4), 2.

\end{thebibliography}

\end{document} 